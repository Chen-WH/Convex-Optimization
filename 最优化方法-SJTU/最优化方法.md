# 最优化方法

## 数学基础

- 范数

$$
非负性 & \|\boldsymbol{x}\|\geq0，等式成立\Leftrightarrow\boldsymbol{x}=\boldsymbol{0}\\
线性 & \|\alpha \boldsymbol{x}\|=|\alpha|\|\boldsymbol{x}\|\\
三角不等式 & \|\boldsymbol{x}+\boldsymbol{y}\|\leq\|\boldsymbol{x}\|+\|\boldsymbol{y}\|
$$

- 矩阵范数

$$
\|\boldsymbol{A}\|_1=\max_{1\leq j\leq n}\sum_{i=1}^n|A_{ij}|\\
\|\boldsymbol{A}\|_\infty=\max_{1\leq i\leq n}\sum_{j=1}^n|A_{ij}|\\
\|\boldsymbol{A}\|_F=\sum_{i,j=1}^n|A_{ij}|\\
\|\boldsymbol{A}\|_2=\sqrt{\sum_{i,j=1}^nA_{ij}^2}
$$

- 算子范数

$$
\|\boldsymbol{A}\|_\vee=\max_{\boldsymbol{x}\not=\boldsymbol{0}}\frac{\|\boldsymbol{Ax}\|_\vee}{\|\boldsymbol{x}\|_\vee}=\max_{\|\boldsymbol{x}\|_\vee=1}\|\boldsymbol{Ax}\|_\vee
$$

$$
\|\boldsymbol{Ax}\|_\vee\leq\|\boldsymbol{A}\|_\vee\|\boldsymbol{x}\|_\vee\\
\|\alpha\boldsymbol{A}\|_\vee=|\alpha|\cdot\|\boldsymbol{A}\|_\vee\\
\|\boldsymbol{A+B}\|_\vee\leq\|\boldsymbol{A}\|_\vee+\|\boldsymbol{B}\|_\vee\\
\|\boldsymbol{AB}\|_\vee\leq\|\boldsymbol{A}\|_\vee\|\boldsymbol{B}\|_\vee
$$

- 序列极限：有界则必有聚点（收敛子列）；Cauchy点列必收敛
- 闭集（S中任意收敛点列的极限还在S中），开集（$\forall \bar x\in S,N(\bar x,\varepsilon)=\{x|\|x-\bar x\|<\varepsilon\}\subseteq S$），紧集（有界闭集）

## 凸集

- 凸集：设 $S\subseteq R^n$，如果对 $\forall x_1,x_2\in S,\forall \alpha\in[0,1],\alpha x_1+(1-\alpha)x_2\in S$

$$
凸集的性质 & S_1,S_2为R^n中的凸集\\
(1) & \beta S_1为凸集\\
(2) & S_1\cap S_2为凸集\\
(3) & S_1\pm S_2为凸集
$$

- 锥：$K\in R^n$，如果对 $\forall x\in K,\lambda\geq0,\lambda x\in K$；追加凸集性质则称为凸锥
- 极点：$S\subseteq R^n$ 非零凸集，$x$ 不能表示成 $S$ 中两个不同点的凸组合
- 方向：$S\subseteq R^n$ 闭凸集，对 $d\in R^n,\forall x\in S,x+\lambda d\in S,\forall \lambda\geq0$（有界集合无方向）；如果 $d_1\not=\alpha d_2,\forall\alpha>0$，则称为两个不同方向；如果不能表示成两个不同方向的正线性组合，则称为极方向
- $S=\{x|Ax=b,x\geq0\}$非空多面集：有限个极点；有界 $\Longleftrightarrow$ 无极方向，无界则有限个极方向
- 凸集分离定理：$S_1,S_2\subseteq R^n,H=\{x|p^Tx=\alpha\}$，如果对 $\forall x\in S_1,p^Tx\geq\alpha,\forall x\in S_2,p^Tx\leq\alpha$，称 $H$ 分离 $S_1,S_2$
- $S_1,S_2$ 凸集 $S_1\cap S_2=\varnothing$，则 $\exist H$ 使得 $H$ 分离 $S_1,S_2$
- 闭凸集外一点与闭凸集的最小距离

$$
\|y-\widetilde x\|=\inf_{x\in S}\|y-x\|\Longleftrightarrow (x-\widetilde x)^T(y-\widetilde x)\leq0
$$

- 点和凸集的分离定理：$S\subseteq R^n$ 非空闭凸集，$y\not\subset S$，则 $\exist p\in R^n,p\not=0$ 使得 $p^Ty>\alpha,p^Tx\leq\alpha,\forall x\in S$ 即 $\exist H:p^Tx=\alpha$ 严格分离 $y,S$ 且 $p^Ty\geq p^Tx+\inf_{x\in S}\|y-x\|^2,\forall x\in S$
- 凸集任意边界点处都存在支撑超平面 $x_0\in\partial S,p^Tx\leq p^Tx_0,\forall x\in S$
- 两个凸集的分离定理：$S_1,S_2\subseteq R^n$ 非空凸集，$S_1\cap S_2=\varnothing$，则存在超平面分离 $S_1,S_2$，即 $\exist p\not=0,p^Tx_1\leq p^Tx_2,\forall x_1\in S_1,x_2\in S_2$

$$
S=S_1-S_2为凸集，且0\not\in S\\
\exist p\not=0,p^Tx\leq\alpha,\forall x\in S,又p^T0\leq\alpha\\
\therefore p^Tx\leq\alpha\leq0\\
p^T(x_1-x_2)\leq0,\forall x_1\in S_1,x_2\in S_2
$$

- Farkas引理

$$
A\in R^{m\times n} & \text{以下两个方程组有且仅有一组有解}\\
(1)Ax\leq0,c^Tx>0,x\in R^n & (2)A^Ty=c,y\geq0,y\in R^m\\
假设(2)有解 & \exist y\in R^m,A^Ty=c,c^Tx=y^TAx=y^T(Ax)\leq0矛盾\\
假设(2)无解 & 令S=\{z|z=A^Ty,\forall y\geq0\}闭凸且非空,c\not\in S\\
& \exist p\not=0,p^Tc>\alpha,p^Tz\leq\alpha,\forall z\in S,又0\in S,p^T0\leq\alpha\\
& \alpha\geq p^Tz=p^TA^Ty=y^TAp,Ap\leq0，有解x=p
$$

-  Gordan引理：$A\in R^{m\times n},Ax<0有解\Longleftrightarrow不存在非零y\geq0,A^Ty=0$

## 凸函数

- 凸函数定义

$$
& 设S\subseteq R^n非空凸集,\alpha\in(0,1)\\
(1)凸函数 &\forall x_1,x_2\in S,f[\alpha x_1+(1-\alpha)x_2]\leq\alpha f(x_1)+(1-\alpha)f(x_2)\\
(2)严格凸函数 & x_1\not=x_2时,f[\alpha x_1+(1-\alpha)x_2]<\alpha f(x_1)+(1-\alpha)f(x_2)\\
(3)一致凸函数 & \exist m>0,f[\alpha x_1+(1-\alpha)x_2]\leq\alpha f(x_1)+(1-\alpha)f(x_2)-m\alpha(1-\alpha)\|x_1-x_2\|^2\\
$$

- 凸规划局部最小值即全局最小值

- 凸函数判别（下述命题等价）

$$
(1) & f是凸函数\\
(2) & \forall x,y\in R^n,f(y)\geq f(x)+\nabla f(x)^T(y-x)\\
(3) & \nabla f单调,\forall x,y\in R^n,(\nabla f(y)-\nabla f(x))^T(y-x)\geq0\\
(4) & \nabla^2f(x)\geq0
$$

- 严格凸的充分非必要条件：$f(x)-f(y)>\nabla f(x)^T(x-y);\nabla^2f(x)>0,\forall x\in S$
- 一致凸 $\Longleftrightarrow \nabla^2f(x)$ 一致正定

## 无约束优化

- 二阶必要条件：若局部极值点，则 $\nabla f(x^*)=0,\nabla^2f(x^*)\geq0$
- 充分条件：$\nabla f(x^*)=0,\nabla^2f(x^*)>0$ 严格局部极小值
- 凸充分性定理：一阶条件即可以等价最优解
- 共轭方向法 $A>0,d_i^TAd_j=0,\forall i\not=j\Longrightarrow d_1,\dots,d_m关于A共轭$
- $d_1,\dots,d_m关于A共轭\Longrightarrow d_1,\dots,d_m是线性无关组$

$$
\alpha_1d_1+\cdots\alpha_md_m=0\\
d_1^TA(\alpha_1d_1+\cdots\alpha_md_m)=0\\
\alpha_1d_1^TAd_1=0,又d_1^TAd_1\not=0\Longrightarrow \alpha_1=0依此类推
$$

- 共轭梯度法

$$
(1) & 给定x_0\in\mathbb{R}^n,d_0=-\nabla f(x_0),\varepsilon\geq0\\
(2) & 若\|\nabla f(x_k)\|\leq\varepsilon,终止\\
(3) & \alpha_k=-\frac{\nabla f(x_k)^Td_k}{d_k^TQd_k}\\
(4) & 令x_{k+1}=x_k+\alpha_kd_k\\
(5) & d_{k+1}=-\nabla f(x_{k+1})+\beta_{k+1}d_k\\
(6) & 令k=k+1,转step2\\
\beta^{HS} & \frac{\nabla f(x_k)^TQd_{k-1}}{d_{k-1}^TQd_{k-1}}=\frac{\nabla f(x_k)^T[\nabla f(x_k)-\nabla f(x_{k-1})]}{d_{k-1}^T[\nabla f(x_k)-\nabla f(x_{k-1})]}\\
\beta^{PRP} & \frac{\nabla f(x_k)^T[\nabla f(x_k)-\nabla f(x_{k-1})]}{\|\nabla f(x_{k-1})\|^2}\\
\beta^{FR} & \frac{\|\nabla f(x_k)\|^2}{\|\nabla f(x_{k-1})\|^2}\\
\beta^{CD} & \frac{\|\nabla f(x_k)\|^2}{d_{k-1}^T\nabla f(x_{k-1})}\\
\beta^{DY} & \frac{\|\nabla f(x_k)\|^2}{d_{k-1}^T[\nabla f(x_k)-\nabla f(x_{k-1})]}
$$

- 拟牛顿法

$$
& y^{(k)T}s^{(k)}>0\Longleftrightarrow B_{k+1}对称正定\\
\Leftarrow & y^{(k)T}s^{(k)}=(B_{k+1}s^{(k)})^Ts^{(k)}>0\\
\Rightarrow & d^TB_{k+1}d=d^TB_kd-\frac{d^TB_k s^{(k)} s^{(k) T} B_kd}{s^{(k) T} B_k s^{(k)}}+\frac{d^Ty^{(k)} y^{(k) T}d}{y^{(k) T} s^{(k)}}\\
& \frac{d^TB_k s^{(k)} s^{(k) T} B_kd}{s^{(k) T} B_k s^{(k)}}=\frac{d^TB_k^{T/2}B_k^{1/2} s^{(k)} s^{(k) T} B_k^{T/2}B_k^{1/2}d}{s^{(k) T} B_k s^{(k)}}\leq\frac{\|B_k^{1/2}d\|^2\|B_k^{1/2} s^{(k)}\|^2}{s^{(k) T} B_k s^{(k)}}=d^TB_{k+1}d
$$

## 非线性最小二乘

- 高斯牛顿法

$$
J=\left[\begin{matrix} \frac{\partial F_1}{\partial x_1} & \cdots & \frac{\partial F_1}{\partial x_n} \\ \vdots & \ddots & \vdots \\ \frac{\partial F_m}{\partial x_1} & \cdots & \frac{\partial F_m}{\partial x_n} \end{matrix}\right]\\
\nabla f(x)=J^T(x)F(x)\\
\nabla^2f(x)=J^T(x)J(x)+\sum_{i=1}^mF_i(x)\nabla^2F_i(x)\\
d_k=-\left[J^T(x)J(x)\right]^{-1}\left[J^T(x)F(x)\right]
$$

$$
h_k=x_k-x^*\\
0=\nabla f(x^*)=\nabla f(x_k)-\nabla^2f(x_k)h_k+o(\|h_k\|^2)\\
J_k^TF_k-(J_k^TJ_k+S_k)h_k+o(\|h_k\|^2)=0\\
当x_k充分靠近x^*时(k充分大)\\
(J_k^TJ_k)^{-1}J_k^TF_k-(J_k^TJ_k)^{-1}(J_k^TJ_k+S_k)h_k+o(\|h_k\|^2)=0\\
-d_k-h_k-(J_k^TJ_k)^{-1}S_kh_k+o(\|h_k\|^2)=0\\
-(x_{k+1}-x^*)-(J_k^TJ_k)^{-1}S_kh_k+o(\|h_k\|^2)=0
$$

- LM法

$$
d_k=-\left[J^T(x)J(x)+\lambda I\right]^{-1}\left[J^T(x)F(x)\right]\\
下降率\rho=\frac{f(x_k)-f(x_k+d_k)}{-J_k^Td_k}\\
下降率好则减小\lambda扩大信赖域半径，反之增大
$$

## 约束非线性规划

- 一阶必要条件

$$
\begin{aligned}
\nabla f(\bar{x})-\sum_{i=1}^m w_i \nabla g_i(\bar{x})-\sum_{j=1}^l v_j \nabla h_j(\bar{x}) &=0 \\
w_i g_i(\bar{x}) &=0, i=1, \ldots, m \\
w_i & \geq 0, i=1, \ldots, m
\end{aligned}
$$

- 二阶条件

$$
\begin{gathered}
d^T \nabla_x^2 L(\bar{x}, \bar{w}, \bar{v}) d \geq 0 \\
\nabla_x^2 L(\bar{x}, \bar{w}, \bar{v})=\nabla^2 f(\bar{x})-\sum_{i=1}^m \bar{w}_i \nabla^2 g_i(\bar{x})-\sum_{j=1}^l \bar{v}_j \nabla^2 h_j(\bar{x})
\end{gathered}
$$

- 二次规划

$$
\min & c^Tx+\frac{1}{2}x^THx\\
s.t. & Ax=b\\
\forall d\in R^n,Ad=0 & d^THd>0\\
Z由A零空间基构成 & d^THd>0\Longleftrightarrow Z^THZ正定
$$

- 惩罚函数法

$$
外点法 & F(x,\sigma)=f(x)+\sigma P(x),\sigma递增\\
& P(x)=\sum[\max\{0,-g_i(x)\}]^\alpha+\sum|h_j(x)|^\beta\\
& 0<\sigma_k<\sigma_{k+1},x^{(k)}和x^{(k+1)}分别为\sigma_k,\sigma_{k+1}时全局极小点\\
条件 & F(x^{(k)},\sigma_k)\leq F(x^{(k+1)},\sigma_k),F(x^{(k+1)},\sigma_{k+1})\leq F(x^{(k)},\sigma_{k+1})\\
(1) & F(x^{(k)},\sigma_k)\leq F(x^{(k+1)},\sigma_{k+1})\\
(2) & P(x^{(k)})\geq P(x^{(k+1)})\\
(3) & f(x^{(k)})\leq f(x^{(k+1)})
$$

$$
内点法 & G(x)=f(x)+rB(x),r递减\\
& B(x)=\sum\frac{1}{g_i(x)}\text{or}-\sum\log g_i(x)
$$

外点法和内点法的优点：均用序列无约束极小化技巧，方法简单，实用方便，并能用来求解约束的导数不存在问题；
缺点：随着惩罚因子趋向其极限，惩罚函数的海森矩阵的条件数无限增大，因而越来越变得病态，惩罚函数的这种性态给无约束极小化带来困难；
乘子法：只要取足够大的惩罚因子，不必趋向无穷大，就可通过极小化子问题求得局部最优解。
$$
\varphi(x,\lambda,\mu,\sigma)=f(x)+\frac{1}{2\sigma}\sum_{i=1}^m\left\{[\max(0,\lambda_i-\sigma g_i(x))]^2-\lambda_i^2\right\}-\sum_{j=1}^l\mu_jh_j(x)+\frac{\sigma}{2}\sum_{j=1}^lh_j^2(x)\\
\lambda_i^{(k+1)}=\max(0,\lambda_i^{(k)}-\sigma g_i(x))\\
\mu_j^{(k+1)}=\mu_j^{(k)}-\sigma h_j(x^{(k)})
$$

- 可行方向法：一阶线性展开
- 投影梯度法：当迭代点在某些约束的边界上时，将该点处的负梯度投影到M的零空间，M是以起作用约束或部分起作用约束的梯度为行构造成的矩阵

$$
P^T=P,P^2=P\Longrightarrow P为投影矩阵,Q=I-P也为投影矩阵\\
\boldsymbol{d}=-\boldsymbol{P}\nabla f(x)=-(\boldsymbol{I-M^T(MM^T)^{-1}M})\nabla f(x)
$$

- 关于子问题（线性规划）的步长搜索

$$
\lambda_{\max }= \begin{cases}\min \left\{\frac{\dot{b}_i}{\hat{d}_i} \mid \hat{d}_i<0\right\}, & \hat{\boldsymbol{d}} \ngtr \mathbf{0}\\ \infty, & \hat{\boldsymbol{d}} \geqslant \mathbf{0} \end{cases}
$$

- 既约梯度法

$$
\boldsymbol{Ax}=\boldsymbol{b}\\
\left[\begin{matrix} \boldsymbol{B} & \boldsymbol{N} \end{matrix}\right]\left[\begin{matrix} \boldsymbol{x}_B \\ \boldsymbol{x}_N \end{matrix}\right]=\boldsymbol{b},\boldsymbol{B}可逆\\
\boldsymbol{x}_B=\boldsymbol{B^{-1}b-B^{-1}Nx}_N\\
既约梯度\boldsymbol{r}(\boldsymbol{x}_N)=\nabla F(\boldsymbol{x}_N)=\nabla_{x_N} f\left(x_B\left(x_N\right), x_N\right)-\left(B^{-1} N\right)^{\mathrm{T}} \nabla_{x_B} f\left(x_B\left(x_N\right), x_N\right)
$$

- Frank-Wolfe方法
